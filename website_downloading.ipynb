{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, requests, time, shutil, glob, bz2, imgkit, re\n",
    "from urllib.parse import urlparse, unquote\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from bs4 import BeautifulSoup\n",
    "from multiprocessing.dummy import Pool\n",
    "from src.azurekey import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'Access-Control-Allow-Origin': '*',\n",
    "    'Access-Control-Allow-Methods': 'GET',\n",
    "    'Access-Control-Allow-Headers': 'Content-Type',\n",
    "    'Access-Control-Max-Age': '3600',\n",
    "    'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0'\n",
    "}\n",
    "\n",
    "def search(search_term, azureKey, offset=0):\n",
    "    # print('searching using bing: \"'+search_term+'\"')\n",
    "    search_url = \"https://api.bing.microsoft.com/v7.0/search\"\n",
    "    subscription_key = azureKey\n",
    "    assert subscription_key\n",
    "    headers = {\"Ocp-Apim-Subscription-Key\" : subscription_key}\n",
    "    params  = {\"q\": search_term, \"responseFilter\": \"Webpages\",\"count\":50, \"offset\":offset, \"safeSearch\":\"Strict\"}\n",
    "    response = requests.get(search_url, headers=headers, params=params)\n",
    "    response.raise_for_status()\n",
    "    search_results = response.json()\n",
    "    return [v[\"url\"] for v in search_results[\"webPages\"][\"value\"]]\n",
    "\n",
    "\n",
    "def save_html2jpg(url, dir_name):\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.mkdir(dir_name)\n",
    "    req = requests.get(url, headers=headers)\n",
    "    imgkit.from_string(clean(req.text), os.path.join(dir_name, str(hash(url))+\".jpg\"))\n",
    "    \n",
    "def repair_link(html, url):\n",
    "    html_temp = re.sub('\"/([^/\"][^<\"]*?)\"', r'\"{}\\1\"'.format(url), html)\n",
    "    html_temp = re.sub(\"'/([^/'][^<']*?)'\", r'\"{}\\1\"'.format(url), html_temp)\n",
    "    html_temp = re.sub(\"url\\(/\", \"url({}\".format(url), html_temp)\n",
    "    return html_temp\n",
    "\n",
    "def download(url, temp_dir, webpage_dir):\n",
    "    save_path = webpage_dir + \"/\" + temp_dir\n",
    "    r = requests.get(url, allow_redirects=True, headers=headers)\n",
    "    file_name = os.path.split(url)[-1]\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    save_file_path = save_path + \"/\" + file_name\n",
    "    open(save_file_path, 'wb').write(r.content)\n",
    "    return temp_dir + \"/\" + file_name\n",
    "\n",
    "def download_all(link_list, url, log, temp_dir=\"temp\", webpage_dir=\"webpages\"):\n",
    "    def fetch(link):\n",
    "        if link in log:\n",
    "            return\n",
    "        if link.startswith(\"http\"):\n",
    "            log[link] = download(link, temp_dir, webpage_dir)\n",
    "        elif link.startswith(\"//\"):\n",
    "            try:\n",
    "                log[link] = download(\"https:\" + link, temp_dir, webpage_dir)\n",
    "            except:\n",
    "                pass\n",
    "        elif link.startswith(\"/\"):\n",
    "            link_temp = os.path.split(url)[0]+link\n",
    "            log[link] = download(link_temp, temp_dir, webpage_dir)\n",
    "            \n",
    "    modLinks=[[link_list[i],i+1] for i in range(len(link_list))]\n",
    "    Pool(10).map(fetch, link_list)\n",
    "    return log\n",
    "\n",
    "def replace_all(html, log):\n",
    "    html_temp = html\n",
    "    for e, v in log.items():\n",
    "        html_temp = re.sub(e, v, html_temp)\n",
    "    return html_temp\n",
    "\n",
    "def repair_tag(html, url, tag):\n",
    "    unquoteURL = unquote(url)\n",
    "    path = urlparse(unquoteURL)\n",
    "    lpath = os.path.dirname(os.path.abspath(path.path))\n",
    "    html_temp = re.sub('{}=\"(//[^h][^t][^<\"]*?)\"'.format(tag), r'{}=\"{}\\1\"'.format(tag, \"{}:\".format(path.scheme)), html)\n",
    "    html_temp = re.sub('{}=\"(/[^h][^t][^<\"]*?)\"'.format(tag), r'{}=\"{}\\1\"'.format(tag, os.path.split(url)[0]), html_temp)\n",
    "    html_temp = re.sub('{}=\"([^h][^t][^<\"]*?)\"'.format(tag), r'{}=\"{}/\\1\"'.format(tag, os.path.split(url)[0]), html_temp)\n",
    "    html_temp = re.sub(\"{}='(//[^h][^t][^<']*?)'\".format(tag), r\"{}='{}\\1'\".format(tag, \"{}:\".format(path.scheme)), html_temp)\n",
    "    html_temp = re.sub(\"{}='(/[^h][^t][^<']*?)'\".format(tag), r\"{}='{}\\1'\".format(tag, os.path.split(url)[0]), html_temp)\n",
    "    html_temp = re.sub(\"{}='([^h][^t][^<']*?)'\".format(tag), r\"{}='{}\\1'\".format(tag, os.path.split(url)[0]), html_temp)\n",
    "    return html_temp\n",
    "                       \n",
    "def repair_link(html, url):\n",
    "    html_temp = repair_tag(html, url, \"src\")\n",
    "    html_temp = repair_tag(html_temp, url, \"href\")\n",
    "    html_temp = repair_tag(html_temp, url, \"poster\")\n",
    "    html_temp = repair_tag(html_temp, url, \"action\")\n",
    "    html_temp = re.sub(\"url\\(/\", \"url({}/\".format(os.path.split(url)[0]), html_temp)\n",
    "    html_temp = re.sub(\"url\\('/\", \"url('{}/\".format(os.path.split(url)[0]), html_temp)\n",
    "    return html_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = search(\"electronics\", azureKey, offset=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.lg.com/fr\n"
     ]
    }
   ],
   "source": [
    "# url = \"https://www.google.com/\"\n",
    "url = search_results[2]\n",
    "req = requests.get(url, headers=headers)\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"webpages/test.html\", \"w\") as f:\n",
    "    f.write(req.text)\n",
    "#     f.write(repair_link(req.text, url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.wired.com/\"\n",
    "req = requests.get(url, headers=headers)\n",
    "html = req.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = {}\n",
    "log = download_all(re.findall('[^ \\'()\"]*?\\.png', req.text), url, log, temp_dir=\"temp\")\n",
    "log = download_all(re.findall('[^ \\'()\"]*?\\.jpg', req.text), url, log, temp_dir=\"temp\")\n",
    "log = download_all(re.findall('[^ \\'()\"]*?\\.js', req.text), url, log, temp_dir=\"temp\")\n",
    "log = download_all(re.findall('[^ \\'()\"]*?\\.css', req.text), url, log, temp_dir=\"temp\")\n",
    "log = download_all(re.findall('[^ \\'()\"]*?\\.json', req.text), url, log, temp_dir=\"temp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Baseurl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.accenture.com/\"\n",
    "req = requests.get(url, headers=headers)\n",
    "html = req.text\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"webpages/test.html\", \"w\") as f:\n",
    "    f.write('<base href=\"{}\">'.format(url))\n",
    "    f.write(soup.prettify())\n",
    "    with open(\"src/blurify.min.js\", \"r\") as f_js:\n",
    "        f.write(\"<script>{}</script>\".format(f_js.readlines()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading page (1/2)\n",
      "[>                                                           ] 0%\r",
      "[======>                                                     ] 10%\r",
      "[======================>                                     ] 38%\r",
      "[=======================>                                    ] 39%\r",
      "[=========================>                                  ] 42%\r",
      "[=========================>                                  ] 42%\r",
      "[===========================>                                ] 45%\r",
      "[============================>                               ] 47%\r",
      "[=============================>                              ] 49%\r",
      "[=============================>                              ] 49%\r",
      "[==============================>                             ] 51%\r",
      "[=================================>                          ] 56%\r",
      "[=====================================>                      ] 63%\r",
      "[========================================>                   ] 68%\r",
      "[===========================================>                ] 72%\r",
      "[==============================================>             ] 77%\r",
      "[=================================================>          ] 82%\r",
      "[=================================================>          ] 82%\r",
      "[=====================================================>      ] 89%\r",
      "[======================================================>     ] 90%\r",
      "[======================================================>     ] 90%\r",
      "[============================================================] 100%\r",
      "Rendering (2/2)                                                    \n",
      "[>                                                           ] 0%\r",
      "[===============>                                            ] 25%\r",
      "[============================================================] 100%\r",
      "Done                                                               \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgkit.from_file(\"webpages/test.html\", \"webpages/test.jpg\", {\"javascript-delay\":\"5000\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APIFlash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_url = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = \"webpages\"\n",
    "url = \"https://www.google.com/\"\n",
    "# search_url = \"https://api.apiflash.com/v1/urltoimage?access_key=37bc0ab0376f4a33b4bac2e7a47776c4&height=1920&js=http%3A%2F%2F35.246.157.75%3A5000%2Fstatic%2Fjs%2Fblurify.js&url={}\".format(url)\n",
    "r=requests.get(search_url, stream=True)\n",
    "r.raw.decode_content = True # handle spurious Content-Encoding\n",
    "img = Image.open(r.raw)\n",
    "img_file = os.path.join(dir_name, str(hash(url))+\".jpg\")\n",
    "img.save(img_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "http://35.246.157.75:5000%2Fstatic%2Fjs%2Fblurify.js\n",
    "http%3A%2F%2F35.246.157.75%3A5000%2Fstatic%2Fjs%2Fblurify.js"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
