{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests, os, imagehash\n",
    "from src.downloader import download\n",
    "from multiprocessing.dummy import Pool\n",
    "\n",
    "def get_html(url):\n",
    "    \"\"\"Get raw HTML from a URL.\"\"\"\n",
    "    headers = {\n",
    "        'Access-Control-Allow-Origin': '*',\n",
    "        'Access-Control-Allow-Methods': 'GET',\n",
    "        'Access-Control-Allow-Headers': 'Content-Type',\n",
    "        'Access-Control-Max-Age': '3600',\n",
    "        'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0'\n",
    "    }\n",
    "    req = requests.get(url, headers=headers)\n",
    "    return req.text\n",
    "\n",
    "def get_image(html):\n",
    "    \"\"\"Scrape share image.\"\"\"\n",
    "    image = []\n",
    "    if html.find(\"meta\", property=\"image\"):\n",
    "        image.append(html.find(\"meta\", property=\"image\").get('content'))\n",
    "    elif html.find(\"meta\", property=\"og:image\"):\n",
    "        image.append(html.find(\"meta\", property=\"og:image\").get('content'))\n",
    "    elif html.find(\"meta\", property=\"twitter:image\"):\n",
    "        image.append(html.find(\"meta\", property=\"twitter:image\").get('content'))\n",
    "    elif html.find(\"img\", src=True):\n",
    "        for img in html.find_all(\"img\"):\n",
    "            image.append(img.get('src'))\n",
    "    return image\n",
    "    \n",
    "def download_images_stream(links, save_path):\n",
    "    headers = {\n",
    "        'Access-Control-Allow-Origin': '*',\n",
    "        'Access-Control-Allow-Methods': 'GET',\n",
    "        'Access-Control-Allow-Headers': 'Content-Type',\n",
    "        'Access-Control-Max-Age': '3600',\n",
    "        'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0'\n",
    "    }\n",
    "    if not os.path.exists(save_path):\n",
    "        os.mkdir(save_path)\n",
    "    def download_stream(link):\n",
    "        try:\n",
    "            r=requests.get(link, headers=headers, stream=True)\n",
    "            r.raw.decode_content = True # handle spurious Content-Encoding\n",
    "            img = Image.open(r.raw)\n",
    "            bits = str(imagehash.average_hash(img))\n",
    "            img_file = os.path.join(save_path, bits+\".jpg\")\n",
    "            img.save(img_file)\n",
    "        except:\n",
    "            pass\n",
    "    Pool(10).map(download_stream, links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob(\"facebook/*.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159 facebook/facebook (5).xlsx\n",
      "167 facebook/facebook (7).xlsx\n",
      "265 facebook/facebook (1).xlsx\n",
      "245 facebook/facebook (6).xlsx\n",
      "31 facebook/facebook.xlsx\n",
      "193 facebook/facebook (2).xlsx\n"
     ]
    }
   ],
   "source": [
    "web_links = []\n",
    "for f in files:\n",
    "    for n in range(5):\n",
    "        try:\n",
    "            df = pd.read_excel(f, sheet_name=n,header = 2)\n",
    "            print(len(df[df[\"type\"]==\"Photo\"]), f)\n",
    "            for link in df[df[\"type\"]==\"Photo\"][\"link\"]:\n",
    "                web_links.append(link)\n",
    "        except:\n",
    "            pass\n",
    "#         try:\n",
    "#             df = pd.read_excel(f, sheet_name=n,header = 2)\n",
    "#             print(len(df[df[\"Type\"]==\"Photo\"]), f)\n",
    "#             for link in df[df[\"Type\"]==\"Photo\"][\"Link\"]:\n",
    "#                 web_links.append(link)\n",
    "#         except:\n",
    "#             pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "298\n"
     ]
    }
   ],
   "source": [
    "print(len(web_links))\n",
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4bdd793cb434c7e957dca878077c7f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=111, max=298)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "import time\n",
    "\n",
    "max_count = len(web_links)\n",
    "f = IntProgress(value=count, min=0, max=max_count) # instantiate the bar\n",
    "display(f) # display the bar\n",
    "for i, link in enumerate(web_links):\n",
    "    if i >= count:\n",
    "        req = requests.get(\"http://0.0.0.0:8889/?url={}\".format(link))\n",
    "        count += 1\n",
    "    f.value += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
